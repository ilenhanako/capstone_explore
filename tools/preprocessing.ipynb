{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC_7x6s9fFQO"
      },
      "source": [
        "# YOLOv11 Complete Training Pipeline\n",
        "## From Raw Merged Dataset to Trained Model\n",
        "\n",
        "**Pipeline:**\n",
        "1. Preprocessing (static masking + label cleaning)\n",
        "2. Dataset analysis & seeding (class imbalance)\n",
        "3. Training with augmentation\n",
        "4. Evaluation with negative class metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLYQ0ZF8fFQQ"
      },
      "source": [
        "## 1. Check GPU & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keGnVZ53fFQR",
        "outputId": "b76144d8-776f-4e41-e55f-7f05d92a23f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 13 18:38:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkDBsE0GfFQR",
        "outputId": "55052118-ff95-46ae-8732-f045a86d375d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q ultralytics albumentations opencv-python-headless matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhA8JKnfFQS"
      },
      "source": [
        "## 2. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXId7S_fFQS",
        "outputId": "275daf05-db50-40cc-fd7f-d1e45fc392a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-bSWl4fFQT"
      },
      "source": [
        "## 3. Upload Pipeline Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lN0cbGmfFQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8L5A0n0fFQU"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration.\"\"\"\n",
        "\n",
        "    # ==================== PATHS (EDIT THESE) ====================\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "    RAW_DATASET_PATH = f\"{DRIVE_ROOT}/mergedDataset\"  # Your raw merged dataset (already split)\n",
        "    PROCESSED_DATASET_PATH = f\"{DRIVE_ROOT}/preprocessedDataset\"  # After preprocessing\n",
        "    OUTPUT_PATH = f\"{DRIVE_ROOT}/yolo_training_runs\"\n",
        "\n",
        "    # ==================== PREPROCESSING ====================\n",
        "\n",
        "    # MASKING TO REMOVE MEANINGLESS NOISE. PLUS INCREASE SPEED FOR TRAINING.\n",
        "    TOP_MASK = 0.20  # Mask top for sky,\n",
        "    BOTTOM_MASK = 0.08  # Mask bottom for hood, dashboard etc\n",
        "\n",
        "    # REMOVE LABELS THAT LIES BEYOND THE MASKING (might need to revise this)\n",
        "    MASKED_OVERLAP_THRESHOLD = 0.25  # Drop box if >25% masked\n",
        "    VISIBLE_PARTIAL_THRESHOLD = 0.6  # Mark partial if <60% visible\n",
        "\n",
        "    IGNORE_CLASSES = [3]  # class 3 which is rutting has too little instances\n",
        "    TAXONOMY = {\n",
        "        0: \"road_crack_longitudinal\",\n",
        "        1: \"road_crack_transverse\",\n",
        "        2: \"road_crack_alligator\",\n",
        "        3: \"road_rutting\",  # fitlered out\n",
        "        4: \"pothole\",\n",
        "        5: \"marking_faded\",\n",
        "        6: \"distractor_manhole\",\n",
        "        7: \"distractor_patch\",\n",
        "    }\n",
        "    POSITIVE_CLASSES = [0, 1, 2, 4, 5]\n",
        "    NEGATIVE_CLASSES = [6, 7]  #Distractor classes\n",
        "\n",
        "    # ==================== AUGMENTATION ====================\n",
        "    USE_AUGMENTATION = True\n",
        "    AUGMENTATION_PROB = 0.5  # Apply aug to 50% of images\n",
        "\n",
        "    # ==================== DATASET SEEDING ====================\n",
        "    SEED_STRATEGY = \"oversampling\"  # \"stratified\", \"oversampling\", or \"none\"\n",
        "    OVERSAMPLE_RARE_THRESHOLD = 4000  # Oversample classes with <4000 instances\n",
        "\n",
        "    # ==================== TRAINING ====================\n",
        "    MODEL_SIZE = \"yolov11l\"\n",
        "    PRETRAINED = True\n",
        "    INPUT_SIZE = 640\n",
        "\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 16\n",
        "    WORKERS = 4\n",
        "    PATIENCE = 20\n",
        "\n",
        "    LR0 = 0.001\n",
        "    LRF = 0.01\n",
        "    WARMUP_EPOCHS = 3\n",
        "\n",
        "    CLS_WEIGHT = 0.5\n",
        "    BOX_WEIGHT = 7.5\n",
        "    DFL_WEIGHT = 1.5\n",
        "    NEGATIVE_WEIGHT = 2.0\n",
        "\n",
        "    # ==================== OTHER ====================\n",
        "    RANDOM_SEED = 42\n",
        "    CONF_THRESHOLD = 0.25\n",
        "    IOU_THRESHOLD = 0.45"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NuhDyYqfFQV"
      },
      "source": [
        "## 4. Configure Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKbYMAXTfFQX",
        "outputId": "ac2d98ff-a23f-4b4c-f8c1-f7db5b6f081a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Raw dataset: /content/drive/MyDrive/mergedDataset\n",
            "  Output: /content/drive/MyDrive/yolo_training_runs\n",
            "  Epochs: 100\n",
            "  Batch size: 16\n",
            "  Seeding strategy: stratified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "config = Config()\n",
        "config.RAW_DATASET_PATH = \"/content/drive/MyDrive/mergedDataset\"\n",
        "config.PROCESSED_DATASET_PATH = \"/content/drive/MyDrive/preprocessedDataset\"\n",
        "config.OUTPUT_PATH = \"/content/drive/MyDrive/yolo_training_runs\"\n",
        "\n",
        "# Training settings\n",
        "config.EPOCHS = 100\n",
        "config.BATCH_SIZE = 16\n",
        "config.SEED_STRATEGY = \"stratified\"\n",
        "\n",
        "config.TOP_MASK = 0.20\n",
        "config.BOTTOM_MASK = 0.08\n",
        "config.NEGATIVE_WEIGHT = 2.0\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Raw dataset: {config.RAW_DATASET_PATH}\")\n",
        "print(f\"  Output: {config.OUTPUT_PATH}\")\n",
        "print(f\"  Epochs: {config.EPOCHS}\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Seeding strategy: {config.SEED_STRATEGY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtILRYAfFQX"
      },
      "source": [
        "## 5. Verify Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMSisa6ofFQY",
        "outputId": "03f2912b-61cf-420a-c804-9703a4943b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset structure:\n",
            "total 8.5M\n",
            "-rw------- 1 root root  247 Nov 13 09:11 data.yaml\n",
            "drwx------ 5 root root 4.0K Nov 13 12:21 images\n",
            "drwx------ 2 root root 4.0K Nov 13 12:21 labels\n",
            "-rw------- 1 root root 8.5M Nov 13 09:11 provenance.csv\n",
            "  train: 27514 images\n",
            "  val: 5897 images\n",
            "  test: 5897 images\n",
            "\n",
            "‚úì Dataset structure looks good!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "dataset_path = config.RAW_DATASET_PATH\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "!ls -lh {dataset_path}\n",
        "splits = []\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    img_dir = f\"{dataset_path}/images/{split}\"\n",
        "    if os.path.isdir(img_dir):\n",
        "        count = len(os.listdir(img_dir))\n",
        "        splits.append(f\"{split}: {count}\")\n",
        "        print(f\"  {split}: {count} images\")\n",
        "\n",
        "if not splits:\n",
        "    print(\"\\n ERROR: No train/val/test splits found\")\n",
        "    print(\"Expected structure:\")\n",
        "    print(\"  mergedDataset/\")\n",
        "    print(\"    images/\")\n",
        "    print(\"      train/\")\n",
        "    print(\"      val/\")\n",
        "    print(\"      test/\")\n",
        "    print(\"    labels/\")\n",
        "    print(\"      train/\")\n",
        "    print(\"      val/\")\n",
        "    print(\"      test/\")\n",
        "else:\n",
        "    print(\"\\n‚úì Dataset structure looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E4p-dngfFQZ"
      },
      "source": [
        "## 6. Run Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx1bv5CtfFQZ",
        "outputId": "4ea81ec0-bf9f-41b1-f8e0-2bbb6d19865a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\n",
            "======================================================================\n",
            "               YOLOv11 COMPLETE TRAINING PIPELINE\n",
            "======================================================================\n",
            "üìÅ Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Google Drive mounted\n",
            "\n",
            "‚úì Environment setup complete\n",
            "  Device: GPU\n",
            "  GPU: Tesla T4\n",
            "\n",
            "============================================================\n",
            "STEP 1: PREPROCESSING\n",
            "============================================================\n",
            "Input:  /content/drive/MyDrive/mergedDataset\n",
            "Output: /content/drive/MyDrive/preprocessedDataset\n",
            "Ignoring classes: [3]\n",
            "============================================================\n",
            "\n",
            "Processing train: 27514 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:   2%|‚ñè         | 452/27514 [10:00<8:44:48,  1.16s/it]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete YOLOv11 Training Pipeline for Google Colab\n",
        "From raw merged dataset to trained model.\n",
        "\n",
        "Pipeline:\n",
        "1. Preprocessing (static masking + label cleaning)\n",
        "2. Data augmentation (photometric, geometric, synthetic occlusions)\n",
        "3. Dataset seeding (class imbalance handling)\n",
        "\n",
        "Usage in Colab:\n",
        "    1. Upload this file to Colab\n",
        "    2. Mount Google Drive\n",
        "    3. Run the main() function\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Ultralytics YOLOv11\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Albumentations for augmentation\n",
        "import albumentations as A\n",
        "\n",
        "# =============================================================================\n",
        "# Module 1: Preprocessing\n",
        "# =============================================================================\n",
        "\n",
        "class Preprocessor:\n",
        "    \"\"\"Apply static masking, label cleaning, and class filtering to raw dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.stats = defaultdict(int)\n",
        "\n",
        "    def generate_mask(self, img_height: int, img_width: int) -> np.ndarray:\n",
        "        \"\"\"Generate binary mask (0=keep, 1=masked).\"\"\"\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "        top_h = int(self.config.TOP_MASK * img_height)\n",
        "        bottom_h = int(self.config.BOTTOM_MASK * img_height)\n",
        "\n",
        "        mask[:top_h, :] = 1\n",
        "        mask[img_height - bottom_h:, :] = 1\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def apply_mask(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Black out masked regions.\"\"\"\n",
        "        img_out = img.copy()\n",
        "        img_out[mask > 0] = 0\n",
        "        return img_out\n",
        "\n",
        "    def compute_visible_fraction(self, box: Tuple, mask: np.ndarray,\n",
        "                                 img_w: int, img_h: int) -> float:\n",
        "        \"\"\"Compute visible fraction of a box.\"\"\"\n",
        "        cls_id, xc, yc, w, h = box\n",
        "\n",
        "        x1 = max(0, int((xc - w/2) * img_w))\n",
        "        y1 = max(0, int((yc - h/2) * img_h))\n",
        "        x2 = min(img_w, int((xc + w/2) * img_w))\n",
        "        y2 = min(img_h, int((yc + h/2) * img_h))\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            return 0.0\n",
        "\n",
        "        box_mask = mask[y1:y2, x1:x2]\n",
        "        masked_pixels = (box_mask > 0).sum()\n",
        "        box_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "        return 1.0 - (masked_pixels / max(1, box_area))\n",
        "\n",
        "    def filter_labels(self, boxes: List[Tuple], mask: np.ndarray,\n",
        "                     img_w: int, img_h: int) -> List[Tuple]:\n",
        "        \"\"\"Filter boxes based on mask overlap and ignored classes.\"\"\"\n",
        "        kept = []\n",
        "        drop_threshold = 1.0 - self.config.MASKED_OVERLAP_THRESHOLD\n",
        "\n",
        "        for box in boxes:\n",
        "            cls_id = box[0]\n",
        "\n",
        "            # Filter out ignored classes (e.g., class 3)\n",
        "            if cls_id in self.config.IGNORE_CLASSES:\n",
        "                self.stats['boxes_ignored_class'] += 1\n",
        "                continue\n",
        "\n",
        "            # Filter based on mask overlap\n",
        "            vis_frac = self.compute_visible_fraction(box, mask, img_w, img_h)\n",
        "            if vis_frac >= drop_threshold:\n",
        "                kept.append(box)\n",
        "                self.stats['boxes_kept'] += 1\n",
        "            else:\n",
        "                self.stats['boxes_dropped_mask'] += 1\n",
        "\n",
        "        return kept\n",
        "\n",
        "    def load_yolo_labels(self, label_path: str) -> List[Tuple]:\n",
        "        \"\"\"Load YOLO format labels.\"\"\"\n",
        "        if not os.path.isfile(label_path):\n",
        "            return []\n",
        "\n",
        "        boxes = []\n",
        "        with open(label_path) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                try:\n",
        "                    cls_id = int(parts[0])\n",
        "                    xc, yc, w, h = map(float, parts[1:])\n",
        "                    boxes.append((cls_id, xc, yc, w, h))\n",
        "                    self.stats['boxes_original'] += 1\n",
        "                except ValueError:\n",
        "                    continue\n",
        "        return boxes\n",
        "\n",
        "    def process_split(self, input_dir: str, output_dir: str, split: str):\n",
        "        \"\"\"Process a single split (train/val/test).\"\"\"\n",
        "        input_img_dir = os.path.join(input_dir, \"images\", split)\n",
        "        input_lbl_dir = os.path.join(input_dir, \"labels\", split)\n",
        "        output_img_dir = os.path.join(output_dir, \"images\", split)\n",
        "        output_lbl_dir = os.path.join(output_dir, \"labels\", split)\n",
        "\n",
        "        if not os.path.isdir(input_img_dir):\n",
        "            print(f\"‚ö†Ô∏è  Split '{split}' not found, skipping\")\n",
        "            return\n",
        "\n",
        "        os.makedirs(output_img_dir, exist_ok=True)\n",
        "        os.makedirs(output_lbl_dir, exist_ok=True)\n",
        "\n",
        "        # Find all images\n",
        "        image_files = []\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            image_files.extend(Path(input_img_dir).glob(f\"*{ext}\"))\n",
        "        image_files = sorted(image_files)\n",
        "\n",
        "        print(f\"\\nProcessing {split}: {len(image_files)} images\")\n",
        "\n",
        "        for img_path in tqdm(image_files, desc=f\"{split}\"):\n",
        "            stem = img_path.stem\n",
        "            lbl_path = Path(input_lbl_dir) / f\"{stem}.txt\"\n",
        "\n",
        "            # Load image\n",
        "            img = cv2.imread(str(img_path))\n",
        "            if img is None:\n",
        "                self.stats['images_failed'] += 1\n",
        "                continue\n",
        "\n",
        "            img_h, img_w = img.shape[:2]\n",
        "\n",
        "            # Generate and apply mask\n",
        "            mask = self.generate_mask(img_h, img_w)\n",
        "            img_masked = self.apply_mask(img, mask)\n",
        "\n",
        "            # Load and filter labels\n",
        "            boxes = self.load_yolo_labels(str(lbl_path))\n",
        "            kept_boxes = self.filter_labels(boxes, mask, img_w, img_h)\n",
        "\n",
        "            # Write outputs\n",
        "            out_img_path = Path(output_img_dir) / img_path.name\n",
        "            cv2.imwrite(str(out_img_path), img_masked)\n",
        "\n",
        "            out_lbl_path = Path(output_lbl_dir) / f\"{stem}.txt\"\n",
        "            with open(out_lbl_path, \"w\") as f:\n",
        "                for cls_id, xc, yc, w, h in kept_boxes:\n",
        "                    f.write(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "\n",
        "            self.stats['images_processed'] += 1\n",
        "\n",
        "    def preprocess_dataset(self, input_dir: str, output_dir: str):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 1: PREPROCESSING\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Input:  {input_dir}\")\n",
        "        print(f\"Output: {output_dir}\")\n",
        "        print(f\"Ignoring classes: {self.config.IGNORE_CLASSES}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            self.process_split(input_dir, output_dir, split)\n",
        "\n",
        "        src_yaml = os.path.join(input_dir, \"data.yaml\")\n",
        "        dst_yaml = os.path.join(output_dir, \"data.yaml\")\n",
        "        if os.path.isfile(src_yaml):\n",
        "            shutil.copy2(src_yaml, dst_yaml)\n",
        "        else:\n",
        "            self.create_data_yaml(output_dir)\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(\"PREPROCESSING SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Images processed:          {self.stats['images_processed']}\")\n",
        "        print(f\"Images failed:             {self.stats['images_failed']}\")\n",
        "        print(f\"\\nBox statistics:\")\n",
        "        print(f\"  Original boxes:          {self.stats['boxes_original']}\")\n",
        "        print(f\"  Boxes kept:              {self.stats['boxes_kept']}\")\n",
        "        print(f\"  Boxes dropped (masked):  {self.stats['boxes_dropped_mask']}\")\n",
        "        print(f\"  Boxes ignored (class 3): {self.stats['boxes_ignored_class']}\")\n",
        "        print(f\"  Total filtered:          {self.stats['boxes_dropped_mask'] + self.stats['boxes_ignored_class']}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "    def create_data_yaml(self, output_dir: str):\n",
        "        \"\"\"Create data.yaml.\"\"\"\n",
        "        data_yaml = {\n",
        "            'path': output_dir,\n",
        "            'train': 'images/train',\n",
        "            'val': 'images/val',\n",
        "            'test': 'images/test',\n",
        "            'names': self.config.TAXONOMY,\n",
        "            'nc': len(self.config.TAXONOMY),\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(output_dir, \"data.yaml\"), 'w') as f:\n",
        "            yaml.dump(data_yaml, f, sort_keys=False)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Module 2: Dataset Analysis & Seeding\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetSeeder:\n",
        "    \"\"\"Analyze class distribution and apply seeding strategies.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.class_distribution = None\n",
        "        self.image_class_map = {}\n",
        "\n",
        "    def analyze_distribution(self, dataset_path: str, split: str = \"train\"):\n",
        "        \"\"\"Analyze class distribution.\"\"\"\n",
        "        labels_dir = os.path.join(dataset_path, \"labels\", split)\n",
        "\n",
        "        class_counts = Counter()\n",
        "        image_class_map = {}\n",
        "\n",
        "        for label_file in Path(labels_dir).glob(\"*.txt\"):\n",
        "            with open(label_file) as f:\n",
        "                classes = []\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        cls_id = int(line.split()[0])\n",
        "                        class_counts[cls_id] += 1\n",
        "                        classes.append(cls_id)\n",
        "\n",
        "                if classes:\n",
        "                    image_class_map[label_file.stem] = list(set(classes))\n",
        "\n",
        "        self.class_distribution = dict(class_counts)\n",
        "        self.image_class_map = image_class_map\n",
        "\n",
        "        return class_counts\n",
        "\n",
        "    def print_distribution(self):\n",
        "        \"\"\"Print class distribution report.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CLASS DISTRIBUTION ANALYSIS (After Filtering)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not self.class_distribution:\n",
        "            print(\"No distribution data available\")\n",
        "            return\n",
        "\n",
        "        total = sum(self.class_distribution.values())\n",
        "\n",
        "        print(f\"\\n{'Class':<5} {'Name':<30} {'Count':>8} {'%':>7} {'Type':>12}\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        for cls_id in sorted(self.class_distribution.keys()):\n",
        "            count = self.class_distribution[cls_id]\n",
        "            pct = 100 * count / total\n",
        "            name = self.config.TAXONOMY.get(cls_id, f\"unknown_{cls_id}\")\n",
        "\n",
        "            if cls_id in self.config.IGNORE_CLASSES:\n",
        "                type_str = \"IGNORED\"\n",
        "            elif cls_id in self.config.NEGATIVE_CLASSES:\n",
        "                type_str = \"NEGATIVE\"\n",
        "            else:\n",
        "                type_str = \"POSITIVE\"\n",
        "\n",
        "            print(f\"{cls_id:<5} {name:<30} {count:>8} {pct:>6.2f}% {type_str:>12}\")\n",
        "\n",
        "        print(\"-\"*60)\n",
        "        print(f\"{'TOTAL':<5} {'':<30} {total:>8} {'100.00%':>7}\")\n",
        "        print(f\"\\nTotal images: {len(self.image_class_map)}\")\n",
        "\n",
        "        # Check if class 3 still exists(it shouldn't after filtering)\n",
        "        if 3 in self.class_distribution:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: Class 3 (road_rutting) still present with {self.class_distribution[3]} boxes!\")\n",
        "            print(\"   This shouldn't happen after preprocessing. Check IGNORE_CLASSES config.\")\n",
        "        else:\n",
        "            print(f\"\\n‚úì Class 3 (road_rutting) successfully filtered out\")\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def plot_distribution(self, output_dir: str):\n",
        "        \"\"\"Plot class distribution.\"\"\"\n",
        "        if not self.class_distribution:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(14, 7))\n",
        "\n",
        "        classes = sorted(self.class_distribution.keys())\n",
        "        counts = [self.class_distribution[c] for c in classes]\n",
        "\n",
        "        # Color coding\n",
        "        colors = []\n",
        "        for c in classes:\n",
        "            if c in self.config.IGNORE_CLASSES:\n",
        "                colors.append('gray')\n",
        "            elif c in self.config.NEGATIVE_CLASSES:\n",
        "                colors.append('red')\n",
        "            else:\n",
        "                colors.append('green')\n",
        "\n",
        "        labels = [self.config.TAXONOMY.get(c, f\"Class {c}\") for c in classes]\n",
        "\n",
        "        bars = plt.bar(range(len(classes)), counts, color=colors, alpha=0.7)\n",
        "        plt.xlabel('Class', fontsize=12)\n",
        "        plt.ylabel('Number of Boxes', fontsize=12)\n",
        "        plt.title('Class Distribution After Preprocessing\\n(Green=Positive, Red=Negative, Gray=Ignored)', fontsize=14)\n",
        "        plt.xticks(range(len(classes)), labels, rotation=45, ha='right')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add count labels on bars\n",
        "        for i, (bar, count) in enumerate(zip(bars, counts)):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{count}',\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plot_path = os.path.join(output_dir, \"class_distribution.png\")\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"\\n‚úì Class distribution plot saved: {plot_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def oversample_rare_classes(self, dataset_path: str, output_path: str):\n",
        "        \"\"\"Oversample rare classes by duplicating images.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 2: DATASET SEEDING (Oversampling)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if self.config.SEED_STRATEGY != \"oversampling\":\n",
        "            print(\"Skipping oversampling (strategy not enabled)\")\n",
        "            return dataset_path\n",
        "\n",
        "        # Find rare classes (excluding ignored classes)\n",
        "        active_classes = {\n",
        "            cls_id: count\n",
        "            for cls_id, count in self.class_distribution.items()\n",
        "            if cls_id not in self.config.IGNORE_CLASSES\n",
        "        }\n",
        "\n",
        "        if not active_classes:\n",
        "            print(\"No active classes found\")\n",
        "            return dataset_path\n",
        "\n",
        "        max_count = max(active_classes.values())\n",
        "        rare_classes = {\n",
        "            cls_id: max(2, min(10, max_count // count))  # Cap replication at 10x\n",
        "            for cls_id, count in active_classes.items()\n",
        "            if count < self.config.OVERSAMPLE_RARE_THRESHOLD\n",
        "        }\n",
        "\n",
        "        if not rare_classes:\n",
        "            print(\"No rare classes found, skipping oversampling\")\n",
        "            return dataset_path\n",
        "\n",
        "        print(f\"\\nRare classes to oversample:\")\n",
        "        for cls_id, factor in rare_classes.items():\n",
        "            name = self.config.TAXONOMY.get(cls_id, f\"Class {cls_id}\")\n",
        "            current = self.class_distribution[cls_id]\n",
        "            print(f\"  Class {cls_id} ({name}): {current} boxes ‚Üí {factor}x replication\")\n",
        "\n",
        "        # Copy dataset with oversampling (train only)\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            src_img_dir = os.path.join(dataset_path, \"images\", split)\n",
        "            src_lbl_dir = os.path.join(dataset_path, \"labels\", split)\n",
        "            dst_img_dir = os.path.join(output_path, \"images\", split)\n",
        "            dst_lbl_dir = os.path.join(output_path, \"labels\", split)\n",
        "\n",
        "            if not os.path.isdir(src_img_dir):\n",
        "                continue\n",
        "\n",
        "            os.makedirs(dst_img_dir, exist_ok=True)\n",
        "            os.makedirs(dst_lbl_dir, exist_ok=True)\n",
        "\n",
        "            copy_count = 0\n",
        "\n",
        "            # Only oversample train split\n",
        "            apply_oversampling = (split == \"train\")\n",
        "\n",
        "            # Get all images in this split\n",
        "            all_images = {}\n",
        "            for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                for img_path in Path(src_img_dir).glob(f\"*{ext}\"):\n",
        "                    all_images[img_path.stem] = img_path\n",
        "\n",
        "            for img_stem, img_path in all_images.items():\n",
        "                src_lbl = os.path.join(src_lbl_dir, f\"{img_stem}.txt\")\n",
        "\n",
        "                # Get classes in this image\n",
        "                image_classes = []\n",
        "                if os.path.exists(src_lbl):\n",
        "                    with open(src_lbl) as f:\n",
        "                        for line in f:\n",
        "                            if line.strip():\n",
        "                                image_classes.append(int(line.split()[0]))\n",
        "\n",
        "                # Determine replication factor\n",
        "                replication = 1\n",
        "                if apply_oversampling:\n",
        "                    for cls_id in image_classes:\n",
        "                        if cls_id in rare_classes:\n",
        "                            replication = max(replication, rare_classes[cls_id])\n",
        "\n",
        "                # Copy with replication\n",
        "                for i in range(replication):\n",
        "                    suffix = f\"_rep{i}\" if i > 0 else \"\"\n",
        "                    dst_img = os.path.join(dst_img_dir, f\"{img_stem}{suffix}{img_path.suffix}\")\n",
        "                    dst_lbl = os.path.join(dst_lbl_dir, f\"{img_stem}{suffix}.txt\")\n",
        "\n",
        "                    shutil.copy2(img_path, dst_img)\n",
        "                    if os.path.exists(src_lbl):\n",
        "                        shutil.copy2(src_lbl, dst_lbl)\n",
        "                    copy_count += 1\n",
        "\n",
        "            original_count = len(all_images)\n",
        "            print(f\"  {split}: {copy_count} images (original: {original_count}, {copy_count/max(1,original_count):.1f}x)\")\n",
        "\n",
        "        # Copy data.yaml\n",
        "        src_yaml = os.path.join(dataset_path, \"data.yaml\")\n",
        "        dst_yaml = os.path.join(output_path, \"data.yaml\")\n",
        "        if os.path.isfile(src_yaml):\n",
        "            shutil.copy2(src_yaml, dst_yaml)\n",
        "\n",
        "        print(f\"\\n‚úì Oversampling complete: {output_path}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return output_path\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Module 3: Training Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "class YOLOTrainer:\n",
        "    \"\"\"YOLOv11 training with augmentation and negative class handling.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "\n",
        "    def setup_environment(self):\n",
        "        \"\"\"Setup Colab environment.\"\"\"\n",
        "        # Check if in Colab\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            print(\"üìÅ Mounting Google Drive...\")\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"‚úì Google Drive mounted\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  Not in Colab, skipping Drive mount\")\n",
        "\n",
        "        # Set random seeds\n",
        "        random.seed(self.config.RANDOM_SEED)\n",
        "        np.random.seed(self.config.RANDOM_SEED)\n",
        "        torch.manual_seed(self.config.RANDOM_SEED)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(self.config.RANDOM_SEED)\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.config.OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "        print(f\"\\n‚úì Environment setup complete\")\n",
        "        print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    def train(self, data_yaml_path: str):\n",
        "        \"\"\"Train YOLOv11 model.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3: TRAINING\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load model\n",
        "        if self.config.PRETRAINED:\n",
        "            print(f\"\\nLoading pretrained {self.config.MODEL_SIZE}...\")\n",
        "            self.model = YOLO(f\"{self.config.MODEL_SIZE}.pt\")\n",
        "        else:\n",
        "            print(f\"\\nInitializing {self.config.MODEL_SIZE} from scratch...\")\n",
        "            self.model = YOLO(f\"{self.config.MODEL_SIZE}.yaml\")\n",
        "\n",
        "        # Training arguments\n",
        "        train_args = {\n",
        "            # Data\n",
        "            'data': data_yaml_path,\n",
        "            'imgsz': self.config.INPUT_SIZE,\n",
        "\n",
        "            # Training\n",
        "            'epochs': self.config.EPOCHS,\n",
        "            'batch': self.config.BATCH_SIZE,\n",
        "            'workers': self.config.WORKERS,\n",
        "            'device': 0 if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "            # Optimization\n",
        "            'optimizer': 'AdamW',\n",
        "            'lr0': self.config.LR0,\n",
        "            'lrf': self.config.LRF,\n",
        "            'momentum': 0.937,\n",
        "            'weight_decay': 0.0005,\n",
        "            'warmup_epochs': self.config.WARMUP_EPOCHS,\n",
        "\n",
        "            # Loss weights\n",
        "            'box': self.config.BOX_WEIGHT,\n",
        "            'cls': self.config.CLS_WEIGHT,\n",
        "            'dfl': self.config.DFL_WEIGHT,\n",
        "\n",
        "            # Augmentation (built-in YOLO augmentations)\n",
        "            'hsv_h': 0.015,\n",
        "            'hsv_s': 0.7,\n",
        "            'hsv_v': 0.4,\n",
        "            'degrees': 5.0,\n",
        "            'translate': 0.1,\n",
        "            'scale': 0.5,\n",
        "            'shear': 2.0,\n",
        "            'flipud': 0.0,\n",
        "            'fliplr': 0.5,\n",
        "            'mosaic': 1.0,\n",
        "            'mixup': 0.1,\n",
        "            'copy_paste': 0.1,\n",
        "\n",
        "            # Validation\n",
        "            'val': True,\n",
        "            'save': True,\n",
        "            'patience': self.config.PATIENCE,\n",
        "\n",
        "            # Output\n",
        "            'project': self.config.OUTPUT_PATH,\n",
        "            'name': 'yolov11l_road_defects',\n",
        "            'exist_ok': True,\n",
        "\n",
        "            # Other\n",
        "            'pretrained': self.config.PRETRAINED,\n",
        "            'verbose': True,\n",
        "            'seed': self.config.RANDOM_SEED,\n",
        "            'cos_lr': True,\n",
        "            'close_mosaic': 10,\n",
        "        }\n",
        "\n",
        "        print(\"\\nüöÄ Starting training...\")\n",
        "        results = self.model.train(**train_args)\n",
        "\n",
        "        print(\"\\n‚úì Training complete!\")\n",
        "        return results\n",
        "\n",
        "    def evaluate(self, data_yaml_path: str):\n",
        "        \"\"\"Evaluate trained model.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4: EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load best model\n",
        "        best_model_path = os.path.join(\n",
        "            self.config.OUTPUT_PATH,\n",
        "            'yolov11l_road_defects',\n",
        "            'weights',\n",
        "            'best.pt'\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(best_model_path):\n",
        "            print(f\"‚ö†Ô∏è  Best model not found: {best_model_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nLoading best model: {best_model_path}\")\n",
        "        eval_model = YOLO(best_model_path)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        test_results = eval_model.val(\n",
        "            data=data_yaml_path,\n",
        "            split='test',\n",
        "            imgsz=self.config.INPUT_SIZE,\n",
        "            batch=self.config.BATCH_SIZE,\n",
        "            conf=self.config.CONF_THRESHOLD,\n",
        "            iou=self.config.IOU_THRESHOLD,\n",
        "            device=0 if torch.cuda.is_available() else 'cpu',\n",
        "        )\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TEST RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"{'Class':<30} {'Precision':>10} {'Recall':>10} {'mAP50':>10}\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        for i, name in test_results.names.items():\n",
        "            if i in self.config.IGNORE_CLASSES:\n",
        "                marker = \"üö´\"\n",
        "            elif i in self.config.NEGATIVE_CLASSES:\n",
        "                marker = \"‚ùå\"\n",
        "            else:\n",
        "                marker = \"‚úÖ\"\n",
        "\n",
        "            p = test_results.box.p[i] if i < len(test_results.box.p) else 0\n",
        "            r = test_results.box.r[i] if i < len(test_results.box.r) else 0\n",
        "            ap = test_results.box.ap50[i] if i < len(test_results.box.ap50) else 0\n",
        "\n",
        "            print(f\"{marker} {name:<27} {p:>10.3f} {r:>10.3f} {ap:>10.3f}\")\n",
        "\n",
        "        print(\"-\"*60)\n",
        "        print(f\"{'Overall mAP50':<30} {test_results.box.map50:>10.3f}\")\n",
        "        print(f\"{'Overall mAP50-95':<30} {test_results.box.map:>10.3f}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return test_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Main Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run complete pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*15 + \"YOLOv11 COMPLETE TRAINING PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    config = Config()\n",
        "    trainer = YOLOTrainer(config)\n",
        "    trainer.setup_environment()\n",
        "\n",
        "    #Preprocessing\n",
        "    preprocessor = Preprocessor(config)\n",
        "    processed_path = preprocessor.preprocess_dataset(\n",
        "        config.RAW_DATASET_PATH,\n",
        "        config.PROCESSED_DATASET_PATH\n",
        "    )\n",
        "\n",
        "    #Analyze & seed dataset\n",
        "    seeder = DatasetSeeder(config)\n",
        "    seeder.analyze_distribution(processed_path, \"train\")\n",
        "    seeder.print_distribution()\n",
        "    seeder.plot_distribution(config.OUTPUT_PATH)\n",
        "\n",
        "    # Apply seeding if enabled\n",
        "    if config.SEED_STRATEGY == \"oversampling\":\n",
        "        seeded_path = os.path.join(config.OUTPUT_PATH, \"seeded_dataset\")\n",
        "        final_dataset_path = seeder.oversample_rare_classes(processed_path, seeded_path)\n",
        "    else:\n",
        "        final_dataset_path = processed_path\n",
        "\n",
        "    data_yaml_path = os.path.join(final_dataset_path, \"data.yaml\")\n",
        "'''\n",
        "    #Train\n",
        "    results = trainer.train(data_yaml_path)\n",
        "\n",
        "    # Step 4: Evaluate\n",
        "    test_results = trainer.evaluate(data_yaml_path)\n",
        "\n",
        "    return trainer, results, test_results '''\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run full pipeline (this will take several hours)\n",
        "# Pipeline steps:\n",
        "#   1. Preprocessing (static masking + label cleaning)\n",
        "#   2. Dataset seeding (class imbalance handling)\n",
        "#   3. Training with augmentation\n",
        "#   4. Evaluation\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hbx4MZ2fFQZ"
      },
      "source": [
        "## 7. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmYUSLDgfFQZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "results_dir = f\"{config.OUTPUT_PATH}/yolov11l_road_defects\"\n",
        "\n",
        "print(\"Class Distribution:\")\n",
        "display(Image(filename=f\"{config.OUTPUT_PATH}/class_distribution.png\"))\n",
        "\n",
        "print(\"\\nTraining Curves:\")\n",
        "display(Image(filename=f\"{results_dir}/results.png\"))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "display(Image(filename=f\"{results_dir}/confusion_matrix.png\"))\n",
        "\n",
        "print(\"\\nPR Curve:\")\n",
        "display(Image(filename=f\"{results_dir}/PR_curve.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJB9O5WqfFQZ"
      },
      "source": [
        "## 8. Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTyqqMM8fFQZ"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load best model\n",
        "best_model = YOLO(f\"{results_dir}/weights/best.pt\")\n",
        "\n",
        "# Get a test image\n",
        "test_img_dir = f\"{config.PROCESSED_DATASET_PATH}/images/test\"\n",
        "test_images = os.listdir(test_img_dir)\n",
        "\n",
        "if test_images:\n",
        "    sample_img = os.path.join(test_img_dir, test_images[0])\n",
        "\n",
        "    # Predict\n",
        "    results = best_model.predict(\n",
        "        sample_img,\n",
        "        conf=0.25,\n",
        "        iou=0.45,\n",
        "        imgsz=640,\n",
        "        save=False,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Visualize\n",
        "    annotated = results[0].plot()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title('Sample Detection Result')\n",
        "    plt.show()\n",
        "\n",
        "    # Print detections\n",
        "    print(\"\\nDetections:\")\n",
        "    for box in results[0].boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        conf = float(box.conf[0])\n",
        "        cls_name = config.TAXONOMY[cls_id]\n",
        "        is_neg = cls_id in config.NEGATIVE_CLASSES\n",
        "        marker = \"‚ùå DISTRACTOR\" if is_neg else \"‚úÖ DEFECT\"\n",
        "        print(f\"  {cls_name}: {conf:.3f}  {marker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Fjb5l2fFQZ"
      },
      "source": [
        "## 9. Export Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxG3NCG7fFQa"
      },
      "outputs": [],
      "source": [
        "# Export to ONNX for deployment\n",
        "best_model.export(format='onnx', imgsz=640, simplify=True)\n",
        "\n",
        "print(f\"\\n‚úì Model exported to: {results_dir}/weights/best.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Y3SBFufFQa"
      },
      "source": [
        "## 10. Download Results (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5yExPhVfFQa"
      },
      "outputs": [],
      "source": [
        "# Compress results for download\n",
        "!cd {config.OUTPUT_PATH} && zip -r results.zip yolov11l_road_defects/weights yolov11l_road_defects/*.png class_distribution.png\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(f'{config.OUTPUT_PATH}/results.zip')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}