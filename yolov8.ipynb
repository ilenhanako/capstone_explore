{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Defect Detection with SVRDD Dataset\n",
    "\n",
    "training an object detection model on the SVRDD dataset.\n",
    "\n",
    "## Project Overview\n",
    "- **Dataset**: SVRDD (Zenodo)\n",
    "- **Classes**: 7 defect types (Alligator Crack, Longitudinal Crack, Transverse Crack, Pothole, Longitudinal Patch, Transverse Patch, Manhole Cover)\n",
    "- **Model**: YOLOv8 for object detection\n",
    "- **Goal**: Detect road defects in real-world footage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics opencv-python pillow matplotlib pandas seaborn\n",
    "!pip install roboflow  # Optional: if using Roboflow for dataset management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download SVRDD Dataset\n",
    "\n",
    "Download the SVRDD dataset from Zenodo: https://zenodo.org/records/4070548\n",
    "\n",
    "License: CC BY 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "SVRDD_DIR = DATA_DIR / \"svrdd\"\n",
    "IMAGES_DIR = SVRDD_DIR / \"images\"\n",
    "ANNOTATIONS_DIR = SVRDD_DIR / \"annotations\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "SVRDD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Dataset directory: {SVRDD_DIR}\")\n",
    "print(\"\\nPlease download the SVRDD dataset from Zenodo and extract it to the above directory.\")\n",
    "print(\"Expected structure:\")\n",
    "print(\"  data/svrdd/\")\n",
    "print(\"    ├── images/\")\n",
    "print(\"    └── annotations/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names based on SVRDD\n",
    "CLASS_NAMES = [\n",
    "    \"alligator_crack\",\n",
    "    \"longitudinal_crack\",\n",
    "    \"transverse_crack\",\n",
    "    \"pothole\",\n",
    "    \"longitudinal_patch\",\n",
    "    \"transverse_patch\",\n",
    "    \"manhole_cover\"\n",
    "]\n",
    "\n",
    "print(f\"Number of classes: {len(CLASS_NAMES)}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "if IMAGES_DIR.exists():\n",
    "    image_files = list(IMAGES_DIR.glob(\"**/*.jpg\")) + list(IMAGES_DIR.glob(\"**/*.png\"))\n",
    "    print(f\"Total images found: {len(image_files)}\")\n",
    "    \n",
    "    # Display sample image\n",
    "    if len(image_files) > 0:\n",
    "        sample_img = Image.open(image_files[0])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(sample_img)\n",
    "        plt.title(f\"Sample Image: {image_files[0].name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Images directory not found at {IMAGES_DIR}\")\n",
    "    print(\"Please download and extract the SVRDD dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert COCO to YOLO Format\n",
    "\n",
    "SVRDD uses COCO format annotations. We need to convert them to YOLO format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_to_yolo_bbox(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert COCO bounding box [x_min, y_min, width, height] to YOLO format [x_center, y_center, width, height]\n",
    "    All values normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    \n",
    "    # Calculate center coordinates\n",
    "    x_center = (x_min + width / 2) / img_width\n",
    "    y_center = (y_min + height / 2) / img_height\n",
    "    \n",
    "    # Normalize width and height\n",
    "    norm_width = width / img_width\n",
    "    norm_height = height / img_height\n",
    "    \n",
    "    return [x_center, y_center, norm_width, norm_height]\n",
    "\n",
    "\n",
    "def convert_coco_to_yolo(coco_json_path, output_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Convert COCO format annotations to YOLO format\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create image id to filename mapping\n",
    "    images_dict = {img['id']: img for img in coco_data['images']}\n",
    "    \n",
    "    # Group annotations by image_id\n",
    "    annotations_by_image = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in annotations_by_image:\n",
    "            annotations_by_image[image_id] = []\n",
    "        annotations_by_image[image_id].append(ann)\n",
    "    \n",
    "    # Convert each image's annotations\n",
    "    for image_id, anns in annotations_by_image.items():\n",
    "        img_info = images_dict[image_id]\n",
    "        img_width = img_info['width']\n",
    "        img_height = img_info['height']\n",
    "        img_filename = Path(img_info['file_name']).stem\n",
    "        \n",
    "        # Create YOLO format txt file\n",
    "        yolo_txt_path = output_dir / f\"{img_filename}.txt\"\n",
    "        \n",
    "        with open(yolo_txt_path, 'w') as f:\n",
    "            for ann in anns:\n",
    "                # COCO category_id is 1-indexed, YOLO class_id is 0-indexed\n",
    "                class_id = ann['category_id'] - 1\n",
    "                bbox = ann['bbox']\n",
    "                \n",
    "                # Convert to YOLO format\n",
    "                yolo_bbox = coco_to_yolo_bbox(bbox, img_width, img_height)\n",
    "                \n",
    "                # Write to file\n",
    "                f.write(f\"{class_id} {' '.join(map(str, yolo_bbox))}\\n\")\n",
    "    \n",
    "    print(f\"Conversion complete! YOLO labels saved to {output_dir}\")\n",
    "    print(f\"Total images processed: {len(annotations_by_image)}\")\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have the COCO annotation file)\n",
    "# convert_coco_to_yolo(\n",
    "#     coco_json_path=\"data/svrdd/annotations/instances_train.json\",\n",
    "#     output_dir=\"data/svrdd/labels/train\",\n",
    "#     images_dir=\"data/svrdd/images/train\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Configuration for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml for YOLO training\n",
    "yaml_content = f\"\"\"# SVRDD Dataset Configuration\n",
    "path: {SVRDD_DIR.absolute()}  # dataset root dir\n",
    "train: images/train  # train images (relative to 'path')\n",
    "val: images/val      # val images (relative to 'path')\n",
    "test: images/test    # test images (optional)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: alligator_crack\n",
    "  1: longitudinal_crack\n",
    "  2: transverse_crack\n",
    "  3: pothole\n",
    "  4: longitudinal_patch\n",
    "  5: transverse_patch\n",
    "  6: manhole_cover\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = SVRDD_DIR / \"svrdd.yaml\"\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Dataset configuration saved to {yaml_path}\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train YOLOv8 Model\n",
    "\n",
    "Starting with YOLOv8s for a balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLOv8 model\n",
    "# Options: yolov8n.pt (nano), yolov8s.pt (small), yolov8m.pt (medium), yolov8l.pt (large), yolov8x.pt (extra large)\n",
    "model = YOLO(\"yolov8s.pt\")  # Start with small model\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: YOLOv8s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'data': str(yaml_path),           # Path to dataset YAML\n",
    "    'epochs': 80,                     # Number of training epochs\n",
    "    'imgsz': 1280,                    # Input image size\n",
    "    'batch': 16,                      # Batch size (adjust based on GPU memory)\n",
    "    'workers': 8,                     # Number of worker threads\n",
    "    'lr0': 0.01,                      # Initial learning rate\n",
    "    'cos_lr': True,                   # Use cosine learning rate scheduler\n",
    "    'patience': 50,                   # Early stopping patience\n",
    "    'save': True,                     # Save training checkpoints\n",
    "    'device': 0,                      # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    'project': 'runs/detect',         # Project directory\n",
    "    'name': 'svrdd_yolov8s',         # Experiment name\n",
    "    \n",
    "    # Augmentation settings for domain gap\n",
    "    'hsv_h': 0.015,                   # HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,                     # HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,                     # HSV-Value augmentation\n",
    "    'degrees': 0.0,                   # Rotation augmentation\n",
    "    'translate': 0.1,                 # Translation augmentation\n",
    "    'scale': 0.5,                     # Scale augmentation\n",
    "    'shear': 0.0,                     # Shear augmentation\n",
    "    'perspective': 0.0,               # Perspective augmentation\n",
    "    'flipud': 0.0,                    # Vertical flip probability\n",
    "    'fliplr': 0.5,                    # Horizontal flip probability\n",
    "    'mosaic': 1.0,                    # Mosaic augmentation probability\n",
    "    'mixup': 0.0,                     # Mixup augmentation probability\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (uncomment when ready)\n",
    "# results = model.train(**TRAINING_CONFIG)\n",
    "\n",
    "print(\"\\nTo start training, uncomment the line above.\")\n",
    "print(\"Training will take several hours depending on your GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for evaluation\n",
    "# trained_model = YOLO(\"runs/detect/svrdd_yolov8s/weights/best.pt\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "# metrics = trained_model.val(data=str(yaml_path), imgsz=1280)\n",
    "\n",
    "# Print metrics\n",
    "# print(f\"\\nValidation Results:\")\n",
    "# print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "# print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "# print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "# print(f\"Recall: {metrics.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference on Custom Video/Images\n",
    "\n",
    "Test the model on your own road footage to validate domain shift handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "# trained_model = YOLO(\"runs/detect/svrdd_yolov8s/weights/best.pt\")\n",
    "\n",
    "# Run inference on custom images\n",
    "# test_image_path = \"path/to/your/test/image.jpg\"\n",
    "# results = trained_model.predict(source=test_image_path, imgsz=1280, conf=0.25, save=True)\n",
    "\n",
    "# Display results\n",
    "# for result in results:\n",
    "#     boxes = result.boxes\n",
    "#     print(f\"Detected {len(boxes)} objects\")\n",
    "#     \n",
    "#     # Plot results\n",
    "#     result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames from video for testing\n",
    "def extract_video_frames(video_path, output_dir, frame_interval=30):\n",
    "    \"\"\"\n",
    "    Extract frames from video at regular intervals\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        output_dir: Directory to save frames\n",
    "        frame_interval: Extract every Nth frame\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = output_dir / f\"frame_{saved_count:06d}.jpg\"\n",
    "            cv2.imwrite(str(frame_filename), frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames from {frame_count} total frames\")\n",
    "    print(f\"Frames saved to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "# extract_video_frames(\"path/to/your/video.mp4\", \"data/test_frames\", frame_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Operational Metrics\n",
    "\n",
    "Track defects per 10 km and false alarms per 10 km as mentioned in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ops_metrics(predictions, distance_km=10, fps=30, speed_kmh=50):\n",
    "    \"\"\"\n",
    "    Calculate operational metrics: defects per 10km and false alarms per 10km\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction results\n",
    "        distance_km: Distance covered in the video (km)\n",
    "        fps: Video frames per second\n",
    "        speed_kmh: Vehicle speed (km/h)\n",
    "    \"\"\"\n",
    "    total_detections = 0\n",
    "    class_counts = {name: 0 for name in CLASS_NAMES}\n",
    "    \n",
    "    for result in predictions:\n",
    "        boxes = result.boxes\n",
    "        total_detections += len(boxes)\n",
    "        \n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_counts[CLASS_NAMES[class_id]] += 1\n",
    "    \n",
    "    # Calculate metrics normalized to 10km\n",
    "    defects_per_10km = (total_detections / distance_km) * 10\n",
    "    \n",
    "    print(f\"\\nOperational Metrics:\")\n",
    "    print(f\"Total defects detected: {total_detections}\")\n",
    "    print(f\"Defects per 10km: {defects_per_10km:.2f}\")\n",
    "    print(f\"\\nPer-class detections:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        per_10km = (count / distance_km) * 10\n",
    "        print(f\"  {class_name}: {count} ({per_10km:.2f} per 10km)\")\n",
    "    \n",
    "    return {\n",
    "        'total_detections': total_detections,\n",
    "        'defects_per_10km': defects_per_10km,\n",
    "        'class_counts': class_counts\n",
    "    }\n",
    "\n",
    "# Example usage after running inference\n",
    "# metrics = calculate_ops_metrics(results, distance_km=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-tune on Custom Data\n",
    "\n",
    "If performance drops on your footage, fine-tune with a small labeled set (300-1000 frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "FINETUNING_CONFIG = {\n",
    "    'data': 'path/to/custom_data.yaml',  # Your custom dataset\n",
    "    'epochs': 40,                         # Fewer epochs for fine-tuning\n",
    "    'imgsz': 1280,\n",
    "    'batch': 16,\n",
    "    'lr0': 0.001,                         # Lower learning rate\n",
    "    'cos_lr': True,\n",
    "    'patience': 20,\n",
    "    'device': 0,\n",
    "    'project': 'runs/detect',\n",
    "    'name': 'svrdd_finetuned',\n",
    "    'resume': False,                      # Start from best weights\n",
    "}\n",
    "\n",
    "# Load best model and fine-tune\n",
    "# best_model = YOLO(\"runs/detect/svrdd_yolov8s/weights/best.pt\")\n",
    "# results = best_model.train(**FINETUNING_CONFIG)\n",
    "\n",
    "print(\"Fine-tuning configuration ready.\")\n",
    "print(\"Prepare your custom dataset with 300-1000 labeled frames first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to different formats for deployment\n",
    "# trained_model = YOLO(\"runs/detect/svrdd_yolov8s/weights/best.pt\")\n",
    "\n",
    "# Export to ONNX (good for cross-platform deployment)\n",
    "# trained_model.export(format='onnx', imgsz=1280)\n",
    "\n",
    "# Export to TensorRT (for NVIDIA GPUs)\n",
    "# trained_model.export(format='engine', imgsz=1280)\n",
    "\n",
    "# Export to CoreML (for Apple devices)\n",
    "# trained_model.export(format='coreml', imgsz=1280)\n",
    "\n",
    "print(\"Available export formats: ONNX, TensorRT, CoreML, TFLite, etc.\")\n",
    "print(\"Choose based on your deployment target hardware.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Download SVRDD dataset from Zenodo\n",
    "2. Convert annotations from COCO to YOLO format\n",
    "3. Train baseline YOLOv8 model\n",
    "4. Test on your custom footage\n",
    "5. Fine-tune if domain gap is significant\n",
    "6. Track both ML metrics (mAP) and operational metrics (defects/10km)\n",
    "7. Consider expanding with RDD2022/N-RDD datasets if needed\n",
    "8. Export model for deployment on target hardware\n",
    "\n",
    "## Resources\n",
    "\n",
    "- SVRDD Dataset: https://zenodo.org/records/4070548\n",
    "- Ultralytics YOLOv8 Docs: https://docs.ultralytics.com/\n",
    "- RDD2022 Dataset: Additional road damage dataset for multi-country data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
