{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Annotation with Grounding DINO (YOLOv11 Format)\n",
    "\n",
    "This notebook uses **Grounding DINO** to automatically annotate road defect videos and export labels in **YOLOv11 format**.\n",
    "\n",
    "## Target Classes:\n",
    "- 0: road_crack_longitudinal\n",
    "- 1: road_crack_transverse\n",
    "- 2: road_crack_alligator\n",
    "- 3: road_rutting\n",
    "- 4: pothole\n",
    "- 5: marking_faded\n",
    "- 6: distractor_manhole\n",
    "- 7: distractor_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision pillow opencv-python matplotlib numpy timm tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Grounding DINO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"  # Use 'grounding-dino-base' for better accuracy\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Class Mapping and Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping (YOLO format)\n",
    "CLASS_MAP = {\n",
    "    0: \"road_crack_longitudinal\",\n",
    "    1: \"road_crack_transverse\",\n",
    "    2: \"road_crack_alligator\",\n",
    "    3: \"road_rutting\",\n",
    "    4: \"pothole\",\n",
    "    5: \"marking_faded\",\n",
    "    6: \"distractor_manhole\",\n",
    "    7: \"distractor_patch\"\n",
    "}\n",
    "\n",
    "# Reverse mapping (label text -> class ID)\n",
    "LABEL_TO_CLASS = {\n",
    "    \"longitudinal crack\": 0,\n",
    "    \"longitudinal\": 0,\n",
    "    \"transverse crack\": 1,\n",
    "    \"transverse\": 1,\n",
    "    \"alligator crack\": 2,\n",
    "    \"alligator\": 2,\n",
    "    \"rutting\": 3,\n",
    "    \"rut\": 3,\n",
    "    \"pothole\": 4,\n",
    "    \"hole\": 4,\n",
    "    \"faded marking\": 5,\n",
    "    \"faded\": 5,\n",
    "    \"marking\": 5,\n",
    "    \"manhole\": 6,\n",
    "    \"manhole cover\": 6,\n",
    "    \"patch\": 7,\n",
    "    \"road patch\": 7\n",
    "}\n",
    "\n",
    "# Text prompts for Grounding DINO (use periods to separate)\n",
    "text_prompt = (\n",
    "    \"a longitudinal crack in the road. \"\n",
    "    \"a transverse crack in the road. \"\n",
    "    \"an alligator crack in the pavement. \"\n",
    "    \"road rutting. \"\n",
    "    \"a pothole in the road. \"\n",
    "    \"faded road marking. \"\n",
    "    \"a manhole cover. \"\n",
    "    \"a road patch.\"\n",
    ")\n",
    "\n",
    "print(\"Class Mapping:\")\n",
    "for idx, name in CLASS_MAP.items():\n",
    "    print(f\"  {idx}: {name}\")\n",
    "print(f\"\\nText Prompt: {text_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_class_id(label_text):\n",
    "    label_lower = label_text.lower()\n",
    "    \n",
    "    # Try exact match first\n",
    "    if label_lower in LABEL_TO_CLASS:\n",
    "        return LABEL_TO_CLASS[label_lower]\n",
    "    \n",
    "    # Try partial match (check if any key is in the label)\n",
    "    for key, class_id in LABEL_TO_CLASS.items():\n",
    "        if key in label_lower:\n",
    "            return class_id\n",
    "    \n",
    "    # Default: try to infer from common keywords\n",
    "    if \"longitudinal\" in label_lower:\n",
    "        return 0\n",
    "    elif \"transverse\" in label_lower:\n",
    "        return 1\n",
    "    elif \"alligator\" in label_lower:\n",
    "        return 2\n",
    "    elif \"rut\" in label_lower:\n",
    "        return 3\n",
    "    elif \"pothole\" in label_lower or \"hole\" in label_lower:\n",
    "        return 4\n",
    "    elif \"faded\" in label_lower or \"marking\" in label_lower:\n",
    "        return 5\n",
    "    elif \"manhole\" in label_lower:\n",
    "        return 6\n",
    "    elif \"patch\" in label_lower:\n",
    "        return 7\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def bbox_to_yolo_format(bbox, img_width, img_height):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    \n",
    "    # Calculate center and dimensions\n",
    "    x_center = (x_min + x_max) / 2.0\n",
    "    y_center = (y_min + y_max) / 2.0\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    # Normalize by image dimensions\n",
    "    x_center /= img_width\n",
    "    y_center /= img_height\n",
    "    width /= img_width\n",
    "    height /= img_height\n",
    "    \n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "\n",
    "def save_yolo_annotation(labels, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for class_id, bbox in labels:\n",
    "            # Format: class_id x_center y_center width height\n",
    "            line = f\"{class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "\n",
    "def detect_and_convert_to_yolo(image, text_prompt, threshold=0.3):\n",
    "    # Prepare inputs\n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-process\n",
    "    img_width, img_height = image.size\n",
    "    target_sizes = torch.tensor([[img_height, img_width]]).to(device)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Extract and convert to YOLO format\n",
    "    result = results[0]\n",
    "    boxes = result['boxes'].cpu().numpy()\n",
    "    labels = result['labels']\n",
    "    scores = result['scores'].cpu().numpy()\n",
    "    \n",
    "    yolo_labels = []\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        # Map label to class ID\n",
    "        class_id = map_label_to_class_id(label)\n",
    "        if class_id is None:\n",
    "            print(f\"Warning: Could not map label '{label}' to class ID, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Convert bbox to YOLO format\n",
    "        yolo_bbox = bbox_to_yolo_format(box, img_width, img_height)\n",
    "        yolo_labels.append((class_id, yolo_bbox))\n",
    "    \n",
    "    return yolo_labels, boxes, labels, scores\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image, boxes, labels, scores, threshold=0.3):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    \n",
    "    # Define colors for each class\n",
    "    class_colors = {\n",
    "        0: 'blue',      # longitudinal crack\n",
    "        1: 'green',     # transverse crack\n",
    "        2: 'orange',    # alligator crack\n",
    "        3: 'purple',    # rutting\n",
    "        4: 'red',       # pothole\n",
    "        5: 'yellow',    # faded marking\n",
    "        6: 'cyan',      # manhole\n",
    "        7: 'magenta'    # patch\n",
    "    }\n",
    "    \n",
    "    # Try to load font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        \n",
    "        # Get class ID and color\n",
    "        class_id = map_label_to_class_id(label)\n",
    "        if class_id is None:\n",
    "            continue\n",
    "        \n",
    "        color = class_colors.get(class_id, 'white')\n",
    "        class_name = CLASS_MAP[class_id]\n",
    "        \n",
    "        # Draw box\n",
    "        draw.rectangle(box.tolist(), outline=color, width=2)\n",
    "        \n",
    "        # Draw label\n",
    "        text = f\"{class_name}: {score:.2f}\"\n",
    "        text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
    "        draw.rectangle(text_bbox, fill=color)\n",
    "        draw.text((box[0], box[1] - 15), text, fill='black', font=font)\n",
    "    \n",
    "    return img_draw\n",
    "\n",
    "print(\"Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Video Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_to_yolo_dataset(\n",
    "    video_path,\n",
    "    output_dir,\n",
    "    text_prompt,\n",
    "    frame_interval=1,\n",
    "    threshold=0.3,\n",
    "    max_frames=None,\n",
    "    save_visualizations=True\n",
    "):\n",
    "    # Create output directories\n",
    "    output_dir = Path(output_dir)\n",
    "    images_dir = output_dir / \"images\"\n",
    "    labels_dir = output_dir / \"labels\"\n",
    "    viz_dir = output_dir / \"visualizations\"\n",
    "    \n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if save_visualizations:\n",
    "        viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"Video: {video_path}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Processing every {frame_interval} frame(s)\\n\")\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total_frames_processed': 0,\n",
    "        'total_detections': 0,\n",
    "        'class_counts': defaultdict(int),\n",
    "        'frames_with_detections': 0\n",
    "    }\n",
    "    \n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(total=min(total_frames // frame_interval, max_frames) if max_frames else total_frames // frame_interval)\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Skip frames based on interval\n",
    "            if frame_count % frame_interval != 0:\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check max frames limit\n",
    "            if max_frames and processed_count >= max_frames:\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Run detection and convert to YOLO\n",
    "            yolo_labels, boxes, labels, scores = detect_and_convert_to_yolo(\n",
    "                image, text_prompt, threshold\n",
    "            )\n",
    "            \n",
    "            # Generate filename\n",
    "            frame_name = f\"frame_{processed_count:06d}\"\n",
    "            \n",
    "            # Save image\n",
    "            image_path = images_dir / f\"{frame_name}.jpg\"\n",
    "            image.save(image_path)\n",
    "            \n",
    "            # Save YOLO labels\n",
    "            label_path = labels_dir / f\"{frame_name}.txt\"\n",
    "            save_yolo_annotation(yolo_labels, label_path)\n",
    "            \n",
    "            # Save visualization\n",
    "            if save_visualizations and len(yolo_labels) > 0:\n",
    "                viz_image = visualize_detections(image, boxes, labels, scores, threshold)\n",
    "                viz_path = viz_dir / f\"{frame_name}_viz.jpg\"\n",
    "                viz_image.save(viz_path)\n",
    "            \n",
    "            # Update statistics\n",
    "            stats['total_frames_processed'] += 1\n",
    "            stats['total_detections'] += len(yolo_labels)\n",
    "            if len(yolo_labels) > 0:\n",
    "                stats['frames_with_detections'] += 1\n",
    "            \n",
    "            for class_id, _ in yolo_labels:\n",
    "                stats['class_counts'][class_id] += 1\n",
    "            \n",
    "            processed_count += 1\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        pbar.close()\n",
    "    \n",
    "    # Save dataset.yaml for YOLO\n",
    "    dataset_yaml = {\n",
    "        'path': str(output_dir.absolute()),\n",
    "        'train': 'images',\n",
    "        'val': 'images',\n",
    "        'nc': len(CLASS_MAP),\n",
    "        'names': CLASS_MAP\n",
    "    }\n",
    "    \n",
    "    yaml_path = output_dir / 'dataset.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(dataset_yaml, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Processing Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total frames processed: {stats['total_frames_processed']}\")\n",
    "    print(f\"Total detections: {stats['total_detections']}\")\n",
    "    print(f\"Frames with detections: {stats['frames_with_detections']}\")\n",
    "    print(f\"\\nDetections by class:\")\n",
    "    for class_id in sorted(stats['class_counts'].keys()):\n",
    "        count = stats['class_counts'][class_id]\n",
    "        class_name = CLASS_MAP[class_id]\n",
    "        print(f\"  {class_id} ({class_name}): {count}\")\n",
    "    \n",
    "    print(f\"\\nDataset saved to: {output_dir}\")\n",
    "    print(f\"  - Images: {images_dir}\")\n",
    "    print(f\"  - Labels: {labels_dir}\")\n",
    "    if save_visualizations:\n",
    "        print(f\"  - Visualizations: {viz_dir}\")\n",
    "    print(f\"  - Config: {yaml_path}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Video processing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Video Annotation\n",
    "\n",
    "**Instructions:**\n",
    "1. Update `video_path` to point to your video file\n",
    "2. Adjust `frame_interval` (1 = every frame, 30 = every 30th frame)\n",
    "3. Tune `threshold` (0.2-0.4 typically works well)\n",
    "4. Run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VIDEO_PATH = \"path/to/your/road_defect_video.mp4\"  # UPDATE THIS\n",
    "OUTPUT_DIR = \"./annotated_dataset\"\n",
    "FRAME_INTERVAL = 30  # Process every 30th frame (adjust based on video FPS)\n",
    "THRESHOLD = 0.3  # Detection confidence threshold\n",
    "MAX_FRAMES = 100  # Set to a number to limit processing (e.g., 100 for testing)\n",
    "SAVE_VISUALIZATIONS = True  # Save annotated images\n",
    "\n",
    "# Run annotation\n",
    "stats = process_video_to_yolo_dataset(\n",
    "    video_path=VIDEO_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    text_prompt=text_prompt,\n",
    "    frame_interval=FRAME_INTERVAL,\n",
    "    threshold=THRESHOLD,\n",
    "    max_frames=MAX_FRAMES,\n",
    "    save_visualizations=SAVE_VISUALIZATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Batch Process Multiple Videos\n",
    "\n",
    "Process all videos in a folder automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_videos(\n",
    "    video_dir,\n",
    "    output_base_dir,\n",
    "    text_prompt,\n",
    "    frame_interval=30,\n",
    "    threshold=0.3,\n",
    "    max_frames=None,\n",
    "    save_visualizations=True,\n",
    "    video_extensions=['.mp4', '.avi', '.mov', '.MP4', '.AVI', '.MOV']\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all videos in a directory\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory containing videos\n",
    "        output_base_dir: Base directory for all outputs\n",
    "        text_prompt: Grounding DINO text prompt\n",
    "        frame_interval: Process every Nth frame\n",
    "        threshold: Detection confidence threshold\n",
    "        max_frames: Maximum frames per video (None = all)\n",
    "        save_visualizations: Save annotated images\n",
    "        video_extensions: List of video file extensions to process\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with combined statistics\n",
    "    \"\"\"\n",
    "    video_dir = Path(video_dir)\n",
    "    output_base_dir = Path(output_base_dir)\n",
    "    \n",
    "    # Find all video files\n",
    "    video_files = []\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(video_dir.glob(f\"*{ext}\"))\n",
    "    \n",
    "    video_files = sorted(video_files)\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {video_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(video_files)} video(s) to process:\")\n",
    "    for vf in video_files:\n",
    "        print(f\"  - {vf.name}\")\n",
    "    print()\n",
    "    \n",
    "    # Process each video\n",
    "    all_stats = {}\n",
    "    combined_stats = {\n",
    "        'total_videos': len(video_files),\n",
    "        'total_frames': 0,\n",
    "        'total_detections': 0,\n",
    "        'class_counts': defaultdict(int),\n",
    "        'video_stats': {}\n",
    "    }\n",
    "    \n",
    "    for idx, video_path in enumerate(video_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing video {idx}/{len(video_files)}: {video_path.name}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create output directory for this video\n",
    "        video_name = video_path.stem\n",
    "        output_dir = output_base_dir / video_name\n",
    "        \n",
    "        try:\n",
    "            # Process video\n",
    "            stats = process_video_to_yolo_dataset(\n",
    "                video_path=video_path,\n",
    "                output_dir=output_dir,\n",
    "                text_prompt=text_prompt,\n",
    "                frame_interval=frame_interval,\n",
    "                threshold=threshold,\n",
    "                max_frames=max_frames,\n",
    "                save_visualizations=save_visualizations\n",
    "            )\n",
    "            \n",
    "            # Update combined statistics\n",
    "            combined_stats['total_frames'] += stats['total_frames_processed']\n",
    "            combined_stats['total_detections'] += stats['total_detections']\n",
    "            \n",
    "            for class_id, count in stats['class_counts'].items():\n",
    "                combined_stats['class_counts'][class_id] += count\n",
    "            \n",
    "            combined_stats['video_stats'][video_name] = stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {video_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print combined statistics\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total videos processed: {combined_stats['total_videos']}\")\n",
    "    print(f\"Total frames extracted: {combined_stats['total_frames']}\")\n",
    "    print(f\"Total detections: {combined_stats['total_detections']}\")\n",
    "    print(f\"\\nCombined class distribution:\")\n",
    "    for class_id in sorted(combined_stats['class_counts'].keys()):\n",
    "        count = combined_stats['class_counts'][class_id]\n",
    "        class_name = CLASS_MAP[class_id]\n",
    "        print(f\"  {class_id} ({class_name}): {count}\")\n",
    "    \n",
    "    print(f\"\\nAll datasets saved to: {output_base_dir}\")\n",
    "    \n",
    "    # Create combined dataset.yaml\n",
    "    combined_yaml = {\n",
    "        'path': str(output_base_dir.absolute()),\n",
    "        'train': '*/images',  # All video subdirectories\n",
    "        'val': '*/images',\n",
    "        'nc': len(CLASS_MAP),\n",
    "        'names': CLASS_MAP\n",
    "    }\n",
    "    \n",
    "    yaml_path = output_base_dir / 'combined_dataset.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(combined_yaml, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Combined config: {yaml_path}\")\n",
    "    \n",
    "    return combined_stats\n",
    "\n",
    "print(\"Batch processing function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for batch processing\n",
    "VIDEO_DIR = \"/Users/adelainesuhendro/personal/capstone/data/video\"  # Your video folder\n",
    "OUTPUT_BASE_DIR = \"./annotated_videos\"  # Base output directory\n",
    "FRAME_INTERVAL = 30  # Process every 30th frame (adjust based on video FPS)\n",
    "THRESHOLD = 0.3  # Detection confidence threshold\n",
    "MAX_FRAMES = None  # Set to a number to limit processing per video (e.g., 100 for testing)\n",
    "SAVE_VISUALIZATIONS = True  # Save annotated images for review\n",
    "\n",
    "# Run batch processing on all videos in the folder\n",
    "combined_stats = process_multiple_videos(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    output_base_dir=OUTPUT_BASE_DIR,\n",
    "    text_prompt=text_prompt,\n",
    "    frame_interval=FRAME_INTERVAL,\n",
    "    threshold=THRESHOLD,\n",
    "    max_frames=MAX_FRAMES,\n",
    "    save_visualizations=SAVE_VISUALIZATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on Single Frame\n",
    "\n",
    "Test the annotation pipeline on a single frame before processing the entire video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single test frame\n",
    "test_video_path = \"path/to/your/road_defect_video.mp4\"  # UPDATE THIS\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "\n",
    "# Get a frame from the middle of the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    test_image = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Display original\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(test_image)\n",
    "    plt.title(\"Test Frame\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Frame size: {test_image.size}\")\n",
    "else:\n",
    "    print(\"Could not read frame from video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on test frame\n",
    "yolo_labels, boxes, labels, scores = detect_and_convert_to_yolo(\n",
    "    test_image, text_prompt, threshold=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected {len(yolo_labels)} objects:\\n\")\n",
    "for class_id, bbox in yolo_labels:\n",
    "    class_name = CLASS_MAP[class_id]\n",
    "    print(f\"  Class {class_id} ({class_name}): bbox = {bbox}\")\n",
    "\n",
    "# Visualize\n",
    "viz_image = visualize_detections(test_image, boxes, labels, scores, threshold=0.3)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(viz_image)\n",
    "plt.title(f\"Detections (Threshold: 0.3) - {len(yolo_labels)} objects\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Sample Annotations\n",
    "\n",
    "After processing, view some sample annotated frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display sample annotations\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "viz_dir = output_dir / \"visualizations\"\n",
    "\n",
    "if viz_dir.exists():\n",
    "    viz_files = list(viz_dir.glob(\"*.jpg\"))[:6]  # Show first 6\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, viz_path in enumerate(viz_files):\n",
    "        img = Image.open(viz_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(viz_path.stem)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No visualizations found in {viz_dir}\")\n",
    "    print(\"Run the video annotation first (Section 7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dataset Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the created dataset\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "labels_dir = output_dir / \"labels\"\n",
    "\n",
    "if labels_dir.exists():\n",
    "    label_files = list(labels_dir.glob(\"*.txt\"))\n",
    "    \n",
    "    # Count classes across all annotations\n",
    "    class_counts = defaultdict(int)\n",
    "    total_annotations = 0\n",
    "    files_with_annotations = 0\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if lines:\n",
    "                files_with_annotations += 1\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    class_id = int(line.split()[0])\n",
    "                    class_counts[class_id] += 1\n",
    "                    total_annotations += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'Class ID': class_id,\n",
    "            'Class Name': CLASS_MAP[class_id],\n",
    "            'Count': count,\n",
    "            'Percentage': f\"{count/total_annotations*100:.1f}%\"\n",
    "        }\n",
    "        for class_id, count in sorted(class_counts.items())\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total frames: {len(label_files)}\")\n",
    "    print(f\"Frames with annotations: {files_with_annotations}\")\n",
    "    print(f\"Total annotations: {total_annotations}\")\n",
    "    print(f\"Average annotations per frame: {total_annotations/len(label_files):.2f}\")\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar([CLASS_MAP[cid] for cid in sorted(class_counts.keys())],\n",
    "            [class_counts[cid] for cid in sorted(class_counts.keys())])\n",
    "    plt.xlabel('Defect Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Detected Defects')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Labels directory not found: {labels_dir}\")\n",
    "    print(\"Run the video annotation first (Section 7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tips for Better Results\n",
    "\n",
    "### Threshold Tuning:\n",
    "- **Too high (0.5+)**: May miss valid defects\n",
    "- **Too low (< 0.2)**: Many false positives\n",
    "- **Recommended**: Start at 0.3 and adjust\n",
    "\n",
    "### Text Prompt Engineering:\n",
    "- Be specific: \"longitudinal crack in asphalt\"\n",
    "- Use natural descriptions\n",
    "- Separate with periods\n",
    "- Test different phrasings\n",
    "\n",
    "### Frame Interval:\n",
    "- **1**: Process every frame (slow, redundant)\n",
    "- **15-30**: Good for 30 FPS videos (1-2 frames/sec)\n",
    "- **60**: For high FPS or redundant footage\n",
    "\n",
    "### Model Selection:\n",
    "- **grounding-dino-tiny**: Faster, less accurate\n",
    "- **grounding-dino-base**: Slower, more accurate (recommended)\n",
    "\n",
    "### Post-Processing:\n",
    "- Review visualizations for false positives\n",
    "- Manually correct/remove bad annotations\n",
    "- Use this as pseudo-labels for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "### Option 1: Manual Review\n",
    "1. Review visualizations in `OUTPUT_DIR/visualizations/`\n",
    "2. Manually correct labels in `OUTPUT_DIR/labels/`\n",
    "3. Remove bad frames/annotations\n",
    "\n",
    "### Option 2: Train YOLOv11\n",
    "1. Use this annotated dataset to train YOLOv11\n",
    "2. Split into train/val sets\n",
    "3. Train with: `yolo train data=dataset.yaml model=yolo11n.pt epochs=100`\n",
    "\n",
    "### Option 3: Hybrid Approach\n",
    "1. Use Grounding DINO for initial labeling\n",
    "2. Manually review and correct\n",
    "3. Train YOLOv11 for production use\n",
    "\n",
    "### Using the Dataset:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Train YOLOv11 on your annotated dataset\n",
    "model = YOLO('yolo11n.pt')\n",
    "results = model.train(\n",
    "    data='./annotated_dataset/dataset.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
