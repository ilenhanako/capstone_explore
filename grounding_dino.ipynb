{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Defect Detection with Grounding DINO\n",
    "\n",
    "This notebook demonstrates using **Grounding DINO** for zero-shot road defect detection.\n",
    "\n",
    "## What is Grounding DINO?\n",
    "\n",
    "Grounding DINO is an **open-set object detector** that combines vision and language understanding:\n",
    "\n",
    "- **Zero-shot detection**: Detect objects without training on specific classes\n",
    "- **Text-based queries**: Describe what you want to detect (e.g., \"a pothole. a crack. a manhole cover.\")\n",
    "- **No training required**: Works out of the box with natural language prompts\n",
    "- **State-of-the-art**: 52.5 AP on COCO zero-shot\n",
    "\n",
    "## Grounding DINO vs YOLOv8\n",
    "\n",
    "| Feature | Grounding DINO | YOLOv8 |\n",
    "|---------|----------------|--------|\n",
    "| Detection approach | Zero-shot with text prompts | Trained on specific classes |\n",
    "| Training required | No | Yes (on labeled data) |\n",
    "| Flexibility | Detect anything with text | Limited to trained classes |\n",
    "| Speed | Slower (~10-20 FPS) | Faster (~100+ FPS) |\n",
    "| Accuracy (trained domain) | Lower | Higher |\n",
    "| Accuracy (new domains) | Better generalization | Requires fine-tuning |\n",
    "| Use case | Exploration, prototyping, rare objects | Production, real-time systems |\n",
    "\n",
    "**When to use Grounding DINO:**\n",
    "- Quick prototyping without labeled data\n",
    "- Detecting rare defect types\n",
    "- Exploring new road damage categories\n",
    "- Research and experimentation\n",
    "\n",
    "**When to use YOLOv8:**\n",
    "- Production deployment with labeled data\n",
    "- Real-time processing requirements\n",
    "- Higher accuracy on known defect types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision\n",
    "!pip install pillow opencv-python matplotlib numpy\n",
    "!pip install timm  # Required for Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Grounding DINO Model\n",
    "\n",
    "Available model sizes:\n",
    "- `IDEA-Research/grounding-dino-tiny` - Fastest, lower accuracy\n",
    "- `IDEA-Research/grounding-dino-base` - Balanced (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "# model_id = \"IDEA-Research/grounding-dino-base\"  # Use this for better accuracy\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Road Defect Text Prompts\n",
    "\n",
    "Grounding DINO uses natural language to describe what to detect. Use **periods** to separate different object types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text prompts for road defects\n",
    "# Format: Use periods (.) to separate different objects\n",
    "\n",
    "# Option 1: Simple class names\n",
    "text_prompt_simple = \"pothole. crack. manhole cover. patch.\"\n",
    "\n",
    "# Option 2: Descriptive phrases (often works better)\n",
    "text_prompt_detailed = (\n",
    "    \"a pothole in the road. \"\n",
    "    \"a crack in the pavement. \"\n",
    "    \"an alligator crack. \"\n",
    "    \"a longitudinal crack. \"\n",
    "    \"a transverse crack. \"\n",
    "    \"a road patch. \"\n",
    "    \"a manhole cover.\"\n",
    ")\n",
    "\n",
    "# Option 3: All SVRDD classes\n",
    "text_prompt_svrdd = (\n",
    "    \"alligator crack. \"\n",
    "    \"longitudinal crack. \"\n",
    "    \"transverse crack. \"\n",
    "    \"pothole. \"\n",
    "    \"longitudinal patch. \"\n",
    "    \"transverse patch. \"\n",
    "    \"manhole cover.\"\n",
    ")\n",
    "\n",
    "# Choose which prompt to use\n",
    "text_prompt = text_prompt_svrdd\n",
    "\n",
    "print(f\"Text prompt: {text_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, boxes, labels, scores, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Visualize predictions on image\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        boxes: List of bounding boxes [x_min, y_min, x_max, y_max]\n",
    "        labels: List of label strings\n",
    "        scores: List of confidence scores\n",
    "        threshold: Minimum confidence threshold\n",
    "    \"\"\"\n",
    "    # Create copy of image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Try to load a font, fall back to default if not available\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Define colors for different defect types\n",
    "    colors = {\n",
    "        'pothole': 'red',\n",
    "        'crack': 'yellow',\n",
    "        'alligator': 'orange',\n",
    "        'longitudinal': 'blue',\n",
    "        'transverse': 'green',\n",
    "        'patch': 'purple',\n",
    "        'manhole': 'cyan',\n",
    "        'default': 'white'\n",
    "    }\n",
    "    \n",
    "    # Draw boxes and labels\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        \n",
    "        # Get color based on label\n",
    "        color = colors.get('default', 'white')\n",
    "        for key in colors:\n",
    "            if key in label.lower():\n",
    "                color = colors[key]\n",
    "                break\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw.rectangle(box, outline=color, width=3)\n",
    "        \n",
    "        # Draw label with score\n",
    "        text = f\"{label}: {score:.2f}\"\n",
    "        text_bbox = draw.textbbox((box[0], box[1]), text, font=font)\n",
    "        draw.rectangle(text_bbox, fill=color)\n",
    "        draw.text((box[0], box[1]), text, fill='black', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def postprocess_predictions(outputs, target_sizes, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Post-process model outputs to get boxes, labels, and scores\n",
    "    \"\"\"\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=threshold\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_defects(image, text_prompt, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Detect road defects in an image using text prompts\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or path to image\n",
    "        text_prompt: Text description of objects to detect\n",
    "        threshold: Confidence threshold (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with boxes, labels, and scores\n",
    "    \"\"\"\n",
    "    # Load image if path is provided\n",
    "    if isinstance(image, (str, Path)):\n",
    "        image = Image.open(image)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-process\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = postprocess_predictions(outputs, target_sizes, threshold=threshold)\n",
    "    \n",
    "    # Extract boxes, labels, and scores\n",
    "    result = results[0]\n",
    "    boxes = result['boxes'].cpu().numpy()\n",
    "    labels = result['labels']\n",
    "    scores = result['scores'].cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'boxes': boxes,\n",
    "        'labels': labels,\n",
    "        'scores': scores,\n",
    "        'image': image\n",
    "    }\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test on Sample Image\n",
    "\n",
    "Let's test the model on a sample image (you can replace with your own road images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image (replace with your own road image path)\n",
    "# Option 1: Load from URL\n",
    "sample_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # Replace with road image\n",
    "image = Image.open(requests.get(sample_url, stream=True).raw)\n",
    "\n",
    "# Option 2: Load from local file (uncomment to use)\n",
    "# image = Image.open(\"path/to/your/road/image.jpg\")\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection\n",
    "print(f\"Detecting with prompt: {text_prompt}\\n\")\n",
    "\n",
    "results = detect_defects(\n",
    "    image=image,\n",
    "    text_prompt=text_prompt,\n",
    "    threshold=0.25  # Adjust threshold (0.2-0.4 typically works well)\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(results['boxes'])} detections:\")\n",
    "for label, score in zip(results['labels'], results['scores']):\n",
    "    print(f\"  - {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "annotated_image = image.copy()\n",
    "annotated_image = visualize_predictions(\n",
    "    annotated_image,\n",
    "    results['boxes'],\n",
    "    results['labels'],\n",
    "    results['scores'],\n",
    "    threshold=0.25\n",
    ")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.title(\"Detections\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing on SVRDD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple images from SVRDD dataset\n",
    "data_dir = Path(\"./data/svrdd/images/val\")  # Adjust path as needed\n",
    "\n",
    "if data_dir.exists():\n",
    "    # Get sample images\n",
    "    image_files = list(data_dir.glob(\"*.jpg\"))[:5]  # Process first 5 images\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\\n\")\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        print(f\"Processing {img_path.name}...\")\n",
    "        \n",
    "        # Run detection\n",
    "        results = detect_defects(\n",
    "            image=img_path,\n",
    "            text_prompt=text_prompt,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        \n",
    "        # Visualize\n",
    "        annotated = results['image'].copy()\n",
    "        annotated = visualize_predictions(\n",
    "            annotated,\n",
    "            results['boxes'],\n",
    "            results['labels'],\n",
    "            results['scores'],\n",
    "            threshold=0.3\n",
    "        )\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(annotated)\n",
    "        plt.title(f\"{img_path.name} - {len(results['boxes'])} detections\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Found {len(results['boxes'])} defects\\n\")\n",
    "else:\n",
    "    print(f\"Data directory not found: {data_dir}\")\n",
    "    print(\"Please download and extract the SVRDD dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Video Frames\n",
    "\n",
    "Extract and process frames from road inspection videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_dir, text_prompt, frame_interval=30, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process video and detect road defects\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        output_dir: Directory to save annotated frames\n",
    "        text_prompt: Detection text prompt\n",
    "        frame_interval: Process every Nth frame\n",
    "        threshold: Detection confidence threshold\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    total_detections = 0\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Run detection\n",
    "            results = detect_defects(image, text_prompt, threshold)\n",
    "            \n",
    "            # Visualize\n",
    "            annotated = image.copy()\n",
    "            annotated = visualize_predictions(\n",
    "                annotated,\n",
    "                results['boxes'],\n",
    "                results['labels'],\n",
    "                results['scores'],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            # Save\n",
    "            output_path = output_dir / f\"frame_{processed_count:06d}_detections_{len(results['boxes'])}.jpg\"\n",
    "            annotated.save(output_path)\n",
    "            \n",
    "            total_detections += len(results['boxes'])\n",
    "            processed_count += 1\n",
    "            \n",
    "            if processed_count % 10 == 0:\n",
    "                print(f\"Processed {processed_count} frames, found {total_detections} total detections\")\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total frames: {frame_count}\")\n",
    "    print(f\"Processed frames: {processed_count}\")\n",
    "    print(f\"Total detections: {total_detections}\")\n",
    "    print(f\"Average detections per frame: {total_detections/processed_count:.2f}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "\n",
    "# Example usage (uncomment when you have a video)\n",
    "# process_video(\n",
    "#     video_path=\"path/to/road/video.mp4\",\n",
    "#     output_dir=\"outputs/video_detections\",\n",
    "#     text_prompt=text_prompt_svrdd,\n",
    "#     frame_interval=30,\n",
    "#     threshold=0.3\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment with Different Prompts\n",
    "\n",
    "Try different text descriptions to see what works best for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompts on the same image\n",
    "test_prompts = [\n",
    "    \"damage on road. defect on pavement.\",\n",
    "    \"pothole. crack. hole in road.\",\n",
    "    \"road surface damage. pavement defect.\",\n",
    "    \"broken asphalt. damaged concrete. road crack.\",\n",
    "]\n",
    "\n",
    "# Load test image (replace with your road image)\n",
    "# test_image = Image.open(\"path/to/road/image.jpg\")\n",
    "\n",
    "# for prompt in test_prompts:\n",
    "#     print(f\"\\nTesting prompt: {prompt}\")\n",
    "#     results = detect_defects(test_image, prompt, threshold=0.25)\n",
    "#     print(f\"Detections: {len(results['boxes'])}\")\n",
    "#     \n",
    "#     # Visualize\n",
    "#     annotated = test_image.copy()\n",
    "#     annotated = visualize_predictions(annotated, results['boxes'], results['labels'], results['scores'])\n",
    "#     \n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.imshow(annotated)\n",
    "#     plt.title(f\"Prompt: {prompt}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "print(\"Prompt experimentation code ready. Uncomment to test with your images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Detection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_detection_stats(results_list):\n",
    "    \"\"\"\n",
    "    Calculate statistics from multiple detection results\n",
    "    \n",
    "    Args:\n",
    "        results_list: List of detection result dictionaries\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    \n",
    "    for results in results_list:\n",
    "        all_labels.extend(results['labels'])\n",
    "        all_scores.extend(results['scores'])\n",
    "    \n",
    "    # Count detections by type\n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    stats_df = pd.DataFrame([\n",
    "        {'Defect Type': label, 'Count': count, 'Percentage': f\"{count/len(all_labels)*100:.1f}%\"}\n",
    "        for label, count in label_counts.most_common()\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n=== Detection Statistics ===\")\n",
    "    print(f\"Total detections: {len(all_labels)}\")\n",
    "    print(f\"Unique defect types: {len(label_counts)}\")\n",
    "    print(f\"Average confidence: {np.mean(all_scores):.3f}\")\n",
    "    print(f\"\\nDetection breakdown:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Example usage after processing multiple images\n",
    "# stats = calculate_detection_stats(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tips for Better Results\n",
    "\n",
    "### Prompt Engineering Tips:\n",
    "1. **Be specific**: \"pothole in asphalt\" vs \"pothole\"\n",
    "2. **Use periods**: Separate objects with periods (required)\n",
    "3. **Try synonyms**: \"crack\", \"fissure\", \"split in road\"\n",
    "4. **Add context**: \"damage on road surface\" can work better than just \"damage\"\n",
    "\n",
    "### Threshold Tuning:\n",
    "- **Higher threshold (0.4-0.6)**: Fewer false positives, may miss defects\n",
    "- **Lower threshold (0.2-0.3)**: More detections, more false positives\n",
    "- **Start at 0.3** and adjust based on results\n",
    "\n",
    "### Model Selection:\n",
    "- **grounding-dino-tiny**: Faster, less accurate (good for prototyping)\n",
    "- **grounding-dino-base**: Slower, more accurate (recommended for final results)\n",
    "\n",
    "### Performance:\n",
    "- Grounding DINO is slower than YOLO (10-20 FPS vs 100+ FPS)\n",
    "- Use GPU for acceptable speed\n",
    "- Process every Nth frame for videos (frame_interval=30)\n",
    "- Consider using Grounding DINO for labeling data, then train YOLOv8 for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integration with Segment Anything (SAM)\n",
    "\n",
    "Grounding DINO can be combined with SAM for pixel-accurate segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SAM\n",
    "# !pip install segment-anything\n",
    "\n",
    "# Example workflow (requires SAM setup):\n",
    "# 1. Use Grounding DINO to detect defects (get bounding boxes)\n",
    "# 2. Use SAM to generate masks from those boxes\n",
    "# 3. Get pixel-accurate segmentation of cracks, potholes, etc.\n",
    "\n",
    "print(\"SAM integration can provide pixel-accurate masks for detected defects.\")\n",
    "print(\"This is useful for measuring defect size, calculating area, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Option 1: Use Grounding DINO for Prototyping\n",
    "1. Test on your road footage without training\n",
    "2. Experiment with different text prompts\n",
    "3. Use it to label data for YOLOv8 training\n",
    "\n",
    "### Option 2: Combine with YOLOv8\n",
    "1. Use Grounding DINO to generate pseudo-labels\n",
    "2. Manually verify and correct\n",
    "3. Train YOLOv8 for faster inference\n",
    "\n",
    "### Option 3: Hybrid Approach\n",
    "1. Use YOLOv8 for common defects (fast)\n",
    "2. Use Grounding DINO for rare/new defect types\n",
    "3. Best of both worlds!\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Grounding DINO Paper: https://arxiv.org/abs/2303.05499\n",
    "- Hugging Face Docs: https://huggingface.co/docs/transformers/en/model_doc/grounding-dino\n",
    "- Model Hub: https://huggingface.co/IDEA-Research\n",
    "- Segment Anything: https://segment-anything.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
